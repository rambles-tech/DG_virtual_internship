{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset was pulled from https://www.kaggle.com/datasets/hmavrodiev/sofia-air-quality-dataset/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'data/2017-07_bme280sof.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data with Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read JSON with dask:  0.005956888198852539 sec\n"
     ]
    }
   ],
   "source": [
    "from dask import dataframe as dd\n",
    "start = time.time()\n",
    "dask_df = dd.read_csv(test_path)\n",
    "end = time.time()\n",
    "print(\"Read JSON with dask: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read CSV with pandas:  0.4173860549926758 sec\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "start = time.time()\n",
    "df = pd.read_csv(test_path)\n",
    "end = time.time()\n",
    "print(\"Read CSV with pandas: \",(end-start),\"sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here Dask is better than Pandas, Modin and Ray, with the least reading time of 0.012 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import dataframe as dd\n",
    "df = dd.read_csv(test_path,delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'sensor_id', 'location', 'lat', 'lon', 'timestamp',\n",
       "       'pressure', 'temperature', 'humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[[0]], axis=1)  # df.columns is zero-based pd.Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***DROP THE FIRST COLUMN***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "701548"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No. of Rows\n",
    "len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#No, of Columns\n",
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g1/lrn62zbd6hb4cz0pmbz3jv8c0000gn/T/ipykernel_83506/418848558.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df.columns=df.columns.str.replace('[#,@,&]','')\n"
     ]
    }
   ],
   "source": [
    "# remove special character\n",
    "df.columns=df.columns.str.replace('[#,@,&]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To remove white space from columns\n",
    "df.columns = df.columns.str.replace(' ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sensor_id', 'location', 'lat', 'lon', 'timestamp', 'pressure',\n",
       "       'temperature', 'humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=df.columns\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting utility.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utility.py\n",
    "import logging\n",
    "import os\n",
    "import subprocess\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import gc\n",
    "import re\n",
    "\n",
    "def read_config_file(filepath):\n",
    "    with open(filepath, 'r') as stream:\n",
    "        try:\n",
    "            return yaml.load(stream, Loader=yaml.Loader)\n",
    "        except yaml.YAMLError as exc:\n",
    "            logging.error(exc)\n",
    "\n",
    "def col_header_val(df,table_config):\n",
    "    df = df.drop(df.columns[[0]], axis=1)  # df.columns is zero-based pd.Index\n",
    "    df.columns = df.columns.str.lower()\n",
    "    df.columns = df.columns.str.replace('[^\\w]','_',regex=True)\n",
    "    df.columns = list(map(lambda x: x.strip('_'), list(df.columns)))\n",
    "    expected_col = list(map(lambda x: x.lower(),  table_config['columns']))\n",
    "    received_col = list(map(lambda x: x.lower(), list(df.columns)))\n",
    "    \n",
    "    if len(received_col) == len(expected_col) and set(expected_col)  == set(received_col):\n",
    "        logging.info(\"column name and column length validation passed\")\n",
    "        return 1\n",
    "    else:\n",
    "        logging.info(\"column name and column length validation failed\")\n",
    "        mismatched_columns_file = list(set(df.columns).difference(expected_col))\n",
    "        logging.info(\"Following File columns are not in the YAML file\",mismatched_columns_file)\n",
    "        missing_YAML_file = list(set(expected_col).difference(df.columns))\n",
    "        logging.info(\"Following YAML columns are not in the file uploaded\",missing_YAML_file)\n",
    "        logging.info(f'df columns: {df.columns}')\n",
    "        logging.info(f'expected columns: {expected_col}')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing schema.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile schema.yaml\n",
    "file_type: csv\n",
    "dataset_name: file\n",
    "file_name: 2017-07_bme280sof\n",
    "table_name: sensorData\n",
    "inbound_delimiter: \",\"\n",
    "outbound_delimiter: \"|\"\n",
    "skip_leading_rows: 1\n",
    "columns: \n",
    "    - unnamed__0\n",
    "    - sensor_id\n",
    "    - location\n",
    "    - lat\n",
    "    - lon\n",
    "    - timestamp\n",
    "    - pressure\n",
    "    - temperature\n",
    "    - humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAILED: data/2017-12_sds011sof.csv\n",
      "SUCCESS: data/2018-09_bme280sof.csv\n",
      "FAILED: data/2018-02_sds011sof.csv\n",
      "FAILED: data/2018-06_sds011sof.csv\n",
      "FAILED: data/2018-11_sds011sof.csv\n",
      "SUCCESS: data/2019-04_bme280sof.csv\n",
      "FAILED: data/2018-01_sds011sof.csv\n",
      "SUCCESS: data/2019-03_bme280sof.csv\n",
      "FAILED: data/2018-12_sds011sof.csv\n",
      "FAILED: data/2018-05_sds011sof.csv\n",
      "SUCCESS: data/2019-07_bme280sof.csv\n",
      "FAILED: data/2017-11_sds011sof.csv\n",
      "SUCCESS: data/2017-09_bme280sof.csv\n",
      "FAILED: data/2017-08_sds011sof.csv\n",
      "SUCCESS: data/2017-07_bme280sof.csv\n",
      "SUCCESS: data/2017-10_bme280sof.csv\n",
      "SUCCESS: data/2018-04_bme280sof.csv\n",
      "FAILED: data/2019-06_sds011sof.csv\n",
      "FAILED: data/2019-02_sds011sof.csv\n",
      "SUCCESS: data/2018-10_bme280sof.csv\n",
      "SUCCESS: data/2018-07_bme280sof.csv\n",
      "FAILED: data/2019-05_sds011sof.csv\n",
      "SUCCESS: data/2018-03_bme280sof.csv\n",
      "FAILED: data/2018-08_sds011sof.csv\n",
      "FAILED: data/2019-01_sds011sof.csv\n",
      "SUCCESS: data/2018-02_bme280sof.csv\n",
      "FAILED: data/2018-09_sds011sof.csv\n",
      "FAILED: data/2019-04_sds011sof.csv\n",
      "SUCCESS: data/2018-06_bme280sof.csv\n",
      "SUCCESS: data/2018-11_bme280sof.csv\n",
      "SUCCESS: data/2017-12_bme280sof.csv\n",
      "SUCCESS: data/2017-11_bme280sof.csv\n",
      "FAILED: data/2017-09_sds011sof.csv\n",
      "FAILED: data/2019-03_sds011sof.csv\n",
      "SUCCESS: data/2018-01_bme280sof.csv\n",
      "SUCCESS: data/2018-12_bme280sof.csv\n",
      "SUCCESS: data/2018-05_bme280sof.csv\n",
      "SUCCESS: data/2019-06_bme280sof.csv\n",
      "FAILED: data/2018-04_sds011sof.csv\n",
      "SUCCESS: data/2019-02_bme280sof.csv\n",
      "SUCCESS: data/2017-08_bme280sof.csv\n",
      "FAILED: data/2017-07_sds011sof.csv\n",
      "FAILED: data/2017-10_sds011sof.csv\n",
      "SUCCESS: data/2019-05_bme280sof.csv\n",
      "FAILED: data/2018-10_sds011sof.csv\n",
      "FAILED: data/2018-07_sds011sof.csv\n",
      "SUCCESS: data/2019-01_bme280sof.csv\n",
      "SUCCESS: data/2018-08_bme280sof.csv\n",
      "FAILED: data/2018-03_sds011sof.csv\n"
     ]
    }
   ],
   "source": [
    "#%%writefile process.py\n",
    "\n",
    "import datetime\n",
    "import csv\n",
    "import gzip\n",
    "from dask import dataframe as dd\n",
    "import utility as util\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "logging = False\n",
    "\n",
    "if logging:\n",
    "    import sys\n",
    "    old_stdout = sys.stdout\n",
    "\n",
    "    log_file = open(\"message.log\",\"w\")\n",
    "    sys.stdout = log_file\n",
    "\n",
    "\n",
    "def append_df(df, destination):\n",
    "    # Write csv in gz format in pipe separated text file (|)\n",
    "    df.to_csv(destination,\n",
    "            sep='|',\n",
    "            header=True,\n",
    "            index=False,\n",
    "            quoting=csv.QUOTE_ALL,\n",
    "            compression='gzip',\n",
    "            quotechar='\"',\n",
    "            doublequote=True,\n",
    "            lineterminator='\\n')\n",
    "\n",
    "def get_files_from_warehouse(path):\n",
    "    return glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "def process_data(csv_files, config_data, dest):\n",
    "    \n",
    "    for filename in csv_files:\n",
    "        \n",
    "        df = dd.read_csv(filename,delimiter=config_data['inbound_delimiter'])\n",
    "\n",
    "        #validating the header of the file\n",
    "        if util.col_header_val(df,config_data):\n",
    "\n",
    "            # append data frame to CSV file\n",
    "            append_df(df, dest)\n",
    "\n",
    "            print(f'SUCCESS: {filename}')\n",
    "\n",
    "        else:\n",
    "            print(f'FAILED: {filename}')   \n",
    "\n",
    "config_data = util.read_config_file(\"schema.yaml\")\n",
    "csv_files = get_files_from_warehouse('data/')\n",
    "dest = 'process.csv.gz'\n",
    "\n",
    "process_data(csv_files, config_data, dest)\n",
    "\n",
    "\n",
    "if logging:\n",
    "    sys.stdout = old_stdout\n",
    "    log_file.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the *sds* files have additional columns, `p1` and `p2` which invalidates them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['unnamed__0', 'sensor_id', 'location', 'lat', 'lon', 'timestamp',\n",
       "       'pressure', 'temperature', 'humidity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from dask.delayed import delayed\n",
    "import os\n",
    "\n",
    "target = 'process.csv.gz/'\n",
    "\n",
    "filenames = os.listdir(target)\n",
    "\n",
    "for filename in filenames:\n",
    "\n",
    "    dfs = delayed(pd.read_csv(target+filename, compression = 'gzip', sep='|'))\n",
    "\n",
    "df = dd.from_delayed(dfs) # df is a dask dataframe\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns check out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total rows = 905440\n"
     ]
    }
   ],
   "source": [
    "print(f' Total rows = {len(df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 7811040806\n",
      "Original size: 256\n"
     ]
    }
   ],
   "source": [
    "#compare file sizes\n",
    "\n",
    "total = 0 \n",
    "for file in csv_files:\n",
    "    if 'bme' in file:\n",
    "        total += os.path.getsize(file)\n",
    "\n",
    "print(f'Original size: {total}')\n",
    "\n",
    "\n",
    "compressed = os.path.getsize('process.csv.gz')\n",
    "print(f'Original size: {compressed}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "8b2a96464172119920a0ebac1e3368e3ab36564d6633994c624d2dfa1405f43a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
